{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pVNblDBpVoAq"
   },
   "source": [
    "**Names of all group members:**\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "All code below is only suggestive and you may as well use different approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K1qPukjnSRgc"
   },
   "outputs": [],
   "source": [
    "# Exercise 1.\n",
    "import numpy as np\n",
    "np.random.seed(0)  # for reproducibility\n",
    "\n",
    "# simulate explanatory variables x (the age and monthly income are assumed here to follow a continous uniform distribution)\n",
    "\n",
    "\n",
    "# a) calculate empirical means and standard deviations over training data\n",
    "\n",
    "\n",
    "# b) Can you come up with a few (2 or 3) additional features that may be relevant?\n",
    "# (you don't have to implement those of course, just write down your answer in text)\n",
    "#\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RhR3TshATJOo"
   },
   "outputs": [],
   "source": [
    "# Exercise 2.\n",
    "# Building the datasets:\n",
    "\n",
    "sigmoid = lambda x: 1. / (1. + np.exp(-x))\n",
    "\n",
    "# build the first dataset\n",
    "\n",
    "\n",
    "# build the second dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rz30ywUHUOSG"
   },
   "outputs": [],
   "source": [
    "# Exercise 2. a)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "# \"model = LogisticRegression().fit(X_data, Y_data)\" fits a model\n",
    "# \"pred_X = model.predict_proba(X)\" evaluates the model\n",
    "# (note that it outputs both P(Y=0|X) and P(Y=1|X))\n",
    "# \"log_loss(Y, pred_X)\" evaluates the negative conditional log likelihood (also called cross-entropy loss)\n",
    "\n",
    "# Fit the models on both datasets\n",
    "\n",
    "\n",
    "# Calculate cross-entropy loss on both datasets for train and test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZJawLeRKWIQJ"
   },
   "outputs": [],
   "source": [
    "# Exercise 2. b)\n",
    "# Calculate normalized data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QaQx76yCWOa3"
   },
   "outputs": [],
   "source": [
    "# Exercise 2.b) (i) and (ii)\n",
    "from sklearn.svm import SVC\n",
    "# \"model = SVC(kernel='rbf', gamma=GAMMA, C=C, probability=True)\" creates\n",
    "# a model with kernel exp(-GAMMA \\|x-x'\\|_2^2) and regul. parameter C (note the relation between C and the parameter lambda from the lecture).\n",
    "# \"probability=True\" enables the option \"model.predict_proba(X)\" to predict probabilities from the regression function \\hat{f}^{svm}.\n",
    "# \"model.fit(X, Y)\" optimizes the model parameters (using hinge loss)\n",
    "\n",
    "# Fit the models for both datasets (this can take up to 60 seconds with SVC)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mS2tjfLdYO4Z"
   },
   "outputs": [],
   "source": [
    "# Exercise 2.b (iii)\n",
    "# \"model.predict_proba(X)\" predicts probabilities from features (note that it outputs both P(Y=0|X) and P(Y=1|X))\n",
    "\n",
    "# Calculate cross-entropy loss on both normalized datasets for train and test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2.b (iv)\n",
    "#Would the results change using standardized data instead of normalized data? No need to run any code here, only explain your reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7xTDD8SNZhZz"
   },
   "outputs": [],
   "source": [
    "# Exercise 2.c\n",
    "import matplotlib.pyplot as plt\n",
    "# To calculate the curves, it is fine to take 100 threshold values c, i.e.,\n",
    "ths = np.linspace(0, 1, 100)\n",
    "\n",
    "# To approximately calculate the AUC, it is fine to simply use Riemann sums.\n",
    "# This means, if you have 100 (a_i, b_i) pairs for the curves, a_1 <= a_2 <= ...\n",
    "# then you may simply use the sum\n",
    "# sum_{i=1}^99 (b_i + b_{i+1})/2 * (a_{i+1}-a_i)\n",
    "# as the approximation of the integral (or AUC)\n",
    "\n",
    "\n",
    "# first data set & logistic regression:\n",
    "# (the code should be reusable for all cases, only exchanging datasets and predicted probabilities depending on the model)\n",
    "\n",
    "# Calculate positives (only depending on the dataset)\n",
    "\n",
    "# Calculate true positives for all threshold values\n",
    "\n",
    "# Calculate false positives for all threshold values\n",
    "\n",
    "# Calculate FDR and TPR rate (points on the FDR/TPR curve) and the AUC\n",
    "\n",
    "\n",
    "# second data set & logistic regression:\n",
    "\n",
    "\n",
    "# first data set and rkhs regression:\n",
    "\n",
    "\n",
    "# second data set and rkhs regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5YvGSeDoc9ZS"
   },
   "outputs": [],
   "source": [
    "# Exercise 3.\n",
    "\n",
    "# Set model parameters and define matrix D\n",
    "\n",
    "\n",
    "# Scenario 1:\n",
    "# Define Portfolio and possible outcomes for this portfolio using matrix D\n",
    "\n",
    "\n",
    "# Plot histogram of profits and losses\n",
    "\n",
    "\n",
    "# Calculate expected profit and losses and 95%-VaR\n",
    "\n",
    "\n",
    "# Scenario 2:\n",
    "# Define Portfolio and possible outcomes using the matrix D and the predicted default probabilities from the logistic regression model\n",
    "\n",
    "\n",
    "# Plot histogram of profits and losses\n",
    "\n",
    "\n",
    "# Calculate expected profit and losses and 95%-VaR\n",
    "\n",
    "\n",
    "# Scenario 3:\n",
    "# Define Portfolio and possible outcomes using the matrix D and the predicted default probabilities from the rkhs model\n",
    "\n",
    "\n",
    "# Plot histogram of profits & losses (which is simply the performance of each strategy in scenario k, i.e. a profit if positive or a loss if negative)\n",
    "\n",
    "\n",
    "# Calculate expected profit & losses and 95%-VaR for each strategy"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
