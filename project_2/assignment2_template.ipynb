{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pVNblDBpVoAq"
   },
   "source": [
    "**Names of all group members:**\n",
    "Nils Johnson, Matteo Bodmer, Hai-Yen Van, Lucas Gimeno, Jonas Isler\n",
    "\n",
    "---\n",
    "\n",
    "All code below is only suggestive and you may as well use different approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "K1qPukjnSRgc"
   },
   "outputs": [],
   "source": [
    "# Exercise 1.\n",
    "import numpy as np\n",
    "np.random.seed(0)  # for reproducibility\n",
    "\n",
    "# simulate explanatory variables x (the age and monthly income are assumed here to follow a continous uniform distribution)\n",
    "\n",
    "# training data\n",
    "m = 20000\n",
    "# test data\n",
    "n = 10000\n",
    "\n",
    "# initialize data matrix\n",
    "data = np.zeros((m+n,3))\n",
    "\n",
    "# first feature of data is age between [18,80] sampled from uniform distribution\n",
    "data[:,0] = np.random.uniform(18,80,(m+n))\n",
    "\n",
    "# second feature of data is the monthly income in CHF 1000 in [1,15] sampled from uniform distribution\n",
    "data[:,1] = np.random.uniform(1,15,(m+n))\n",
    "\n",
    "# third feature is a binary probality of being slaraied / self-employed. Probability of being self employed is 10%\n",
    "data[:,2] = np.random.choice([0,1],p=[0.9,0.1],size=(m+n))\n",
    "\n",
    "# a) calculate empirical means and standard deviations over training data\n",
    "\n",
    "mean_train_feature_1 = np.mean(data[:m,0])\n",
    "mean_train_feature_2 = np.mean(data[:m,1])\n",
    "mean_train_feature_3 = np.mean(data[:m,2])\n",
    "\n",
    "std_train_feature_1 = np.std(data[:m,0])\n",
    "std_train_feature_2 = np.std(data[:m,1])\n",
    "std_train_feature_3 = np.std(data[:m,2])\n",
    "\n",
    "# b) Can you come up with a few (2 or 3) additional features that may be relevant?\n",
    "# (you don't have to implement those of course, just write down your answer in text)\n",
    "#\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "RhR3TshATJOo"
   },
   "outputs": [],
   "source": [
    "# Exercise 2.\n",
    "# Building the datasets:\n",
    "\n",
    "sigmoid = lambda x: 1. / (1. + np.exp(-x))\n",
    "\n",
    "x1, x2, x3 = data[:, 0], data[:, 1], data[:, 2]\n",
    "\n",
    "p1 = sigmoid(13.3 - 0.33*x1 + 3.5*x2 - 3*x3)\n",
    "\n",
    "# Indicator: 1 if x1 < 25 or x1 > 75, else 0\n",
    "indicator = ((x1 < 25) | (x1 > 75)).astype(float)\n",
    "\n",
    "p2 = sigmoid(5 - 10*indicator + 1.1*x2 - x3)\n",
    "\n",
    "epsilon = np.random.uniform(size=(m+n))\n",
    "# build the first dataset\n",
    "\n",
    "y_1 = (p1 > epsilon).astype(int)\n",
    "\n",
    "# build the second dataset\n",
    "y_2 = (p2 > epsilon).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Rz30ywUHUOSG"
   },
   "outputs": [],
   "source": [
    "# Exercise 2. a)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "# \"model = LogisticRegression().fit(X_data, Y_data)\" fits a model\n",
    "# \"pred_X = model.predict_proba(X)\" evaluates the model\n",
    "# (note that it outputs both P(Y=0|X) and P(Y=1|X))\n",
    "# \"log_loss(Y, pred_X)\" evaluates the negative conditional log likelihood (also called cross-entropy loss)\n",
    "\n",
    "\n",
    "# Fit the models on both datasets\n",
    "model_1 = LogisticRegression().fit(data[:m,:],y_1[:m])\n",
    "model_2 = LogisticRegression().fit(data[:m,:],y_2[:m])\n",
    "\n",
    "# evaluate both models on the data, we only need the positive class to calculate the cross-entropy loss\n",
    "pred_train_1 = model_1.predict_proba(data[:m,:])[:,1]\n",
    "pred_train_2 = model_2.predict_proba(data[:m,:])[:,1]\n",
    "\n",
    "pred_test_1 = model_1.predict_proba(data[m:,:])[:,1]\n",
    "pred_test_2 = model_2.predict_proba(data[m:,:])[:,1]\n",
    "\n",
    "# Calculate cross-entropy loss on both datasets for train and test\n",
    "cross_entropy_train_1 = log_loss(y_1[:m], pred_train_1)\n",
    "cross_entropy_train_2 = log_loss(y_2[:m], pred_train_2)\n",
    "\n",
    "cross_entropy_test_1 = log_loss(y_1[m:], pred_test_1)\n",
    "cross_entropy_test_2 = log_loss(y_2[m:], pred_test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ZJawLeRKWIQJ"
   },
   "outputs": [],
   "source": [
    "# Exercise 2. b)\n",
    "# Calculate normalized data\n",
    "\n",
    "data_norm = np.zeros(((m+n),3))\n",
    "\n",
    "data_norm[:,0] = data[:,0]/std_train_feature_1\n",
    "data_norm[:,1] = data[:,1]/std_train_feature_2\n",
    "data_norm[:,2] = data[:,2]/std_train_feature_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "QaQx76yCWOa3"
   },
   "outputs": [],
   "source": [
    "# Exercise 2.b) (i) and (ii)\n",
    "from sklearn.svm import SVC\n",
    "# \"model = SVC(kernel='rbf', gamma=GAMMA, C=C, probability=True)\" creates\n",
    "# a model with kernel exp(-GAMMA \\|x-x'\\|_2^2) and regul. parameter C (note the relation between C and the parameter lambda from the lecture).\n",
    "# \"probability=True\" enables the option \"model.predict_proba(X)\" to predict probabilities from the regression function \\hat{f}^{svm}.\n",
    "# \"model.fit(X, Y)\" optimizes the model parameters (using hinge loss)\n",
    "\n",
    "# Fit the models for both datasets (this can take up to 60 seconds with SVC)\n",
    "model_svm_1 = SVC(kernel='rbf', gamma = 0.1, C= 0.2, probability=True).fit(data_norm[:m,:],y_1[:m])\n",
    "model_svm_2 = SVC(kernel='rbf', gamma= 0.1, C=0.2, probability=True).fit(data_norm[:m,:],y_2[:m])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "mS2tjfLdYO4Z"
   },
   "outputs": [],
   "source": [
    "# Exercise 2.b (iii)\n",
    "# \"model.predict_proba(X)\" predicts probabilities from features (note that it outputs both P(Y=0|X) and P(Y=1|X))\n",
    "pred_svm_train_1 = model_svm_1.predict_proba(data_norm[:m,:])[:,1]\n",
    "pred_svm_train_2 = model_svm_2.predict_proba(data_norm[:m,:])[:,1]\n",
    "\n",
    "pred_svm_test_1 = model_svm_1.predict_proba(data_norm[m:,:])[:,1]\n",
    "pred_svm_test_2 = model_svm_2.predict_proba(data_norm[m:,:])[:,1]\n",
    "\n",
    "# Calculate cross-entropy loss on both normalized datasets for train and test\n",
    "cross_entropy_svm_train_1 = log_loss(y_1[:m], pred_svm_train_1)\n",
    "cross_entropy_svm_train_2 = log_loss(y_2[:m], pred_svm_train_2)\n",
    "\n",
    "cross_entropy_svm_test_1 = log_loss(y_1[m:], pred_svm_test_1)\n",
    "cross_entropy_svm_test_2 = log_loss(y_2[m:], pred_svm_test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2.b (iv)\n",
    "#Would the results change using standardized data instead of normalized data? No need to run any code here, only explain your reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7xTDD8SNZhZz"
   },
   "outputs": [],
   "source": [
    "# Exercise 2.c\n",
    "import matplotlib.pyplot as plt\n",
    "# To calculate the curves, it is fine to take 100 threshold values c, i.e.,\n",
    "ths = np.linspace(0, 1, 100)\n",
    "\n",
    "# To approximately calculate the AUC, it is fine to simply use Riemann sums.\n",
    "# This means, if you have 100 (a_i, b_i) pairs for the curves, a_1 <= a_2 <= ...\n",
    "# then you may simply use the sum\n",
    "# sum_{i=1}^99 (b_i + b_{i+1})/2 * (a_{i+1}-a_i)\n",
    "# as the approximation of the integral (or AUC)\n",
    "\n",
    "\n",
    "# first data set & logistic regression:\n",
    "# (the code should be reusable for all cases, only exchanging datasets and predicted probabilities depending on the model)\n",
    "\n",
    "# Calculate positives (only depending on the dataset)\n",
    "\n",
    "# Calculate true positives for all threshold values\n",
    "\n",
    "# Calculate false positives for all threshold values\n",
    "\n",
    "# Calculate FDR and TPR rate (points on the FDR/TPR curve) and the AUC\n",
    "\n",
    "\n",
    "# second data set & logistic regression:\n",
    "\n",
    "\n",
    "# first data set and rkhs regression:\n",
    "\n",
    "\n",
    "# second data set and rkhs regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5YvGSeDoc9ZS"
   },
   "outputs": [],
   "source": [
    "# Exercise 3.\n",
    "\n",
    "# Set model parameters and define matrix D\n",
    "\n",
    "\n",
    "# Scenario 1:\n",
    "# Define Portfolio and possible outcomes for this portfolio using matrix D\n",
    "\n",
    "\n",
    "# Plot histogram of profits and losses\n",
    "\n",
    "\n",
    "# Calculate expected profit and losses and 95%-VaR\n",
    "\n",
    "\n",
    "# Scenario 2:\n",
    "# Define Portfolio and possible outcomes using the matrix D and the predicted default probabilities from the logistic regression model\n",
    "\n",
    "\n",
    "# Plot histogram of profits and losses\n",
    "\n",
    "\n",
    "# Calculate expected profit and losses and 95%-VaR\n",
    "\n",
    "\n",
    "# Scenario 3:\n",
    "# Define Portfolio and possible outcomes using the matrix D and the predicted default probabilities from the rkhs model\n",
    "\n",
    "\n",
    "# Plot histogram of profits & losses (which is simply the performance of each strategy in scenario k, i.e. a profit if positive or a loss if negative)\n",
    "\n",
    "\n",
    "# Calculate expected profit & losses and 95%-VaR for each strategy"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
